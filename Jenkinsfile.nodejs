directory = env.JOB_BASE_NAME
version = ''
artifactName = "${directory}"
extension = 'zip'
path_package = env.JOB_NAME.split('lambda/').last()
path_compress = path_package
content_compress = '*'
s3bucket = 'pechnicki-lambda'
s3prefix = 'functions'
layers = 'layers'
if (path_package.contains(layers)) {
  path_package = "${path_package}/nodejs"
  content_compress = 'nodejs'
  s3prefix = layers
}

pipeline {
  agent any

  options {
    timeout(time: 1, unit: 'HOURS')
    disableConcurrentBuilds()
    buildDiscarder(logRotator(numToKeepStr: '5'))
    timestamps()
  }
  stages {
    stage('BUILD') {
      steps {
        script {
          dir(path_package) {
            version = sh(
              script: "cat package.json | grep version | xargs | cut -d' ' -f2 | sed 's|,||g'",
              returnStdout: true
            ).trim()
            sh 'npm i'
          }
        }
      }
    }
    stage('COMPRESS') {
      steps {
        script {
          dir(path_compress) {
            sh "zip -q -r ${artifactName}-${version}.${extension} ./${content_compress} -x *@tmp/*"
          }
        }
      }
    }
    stage('UPLOAD TO S3') {
      steps {
        script {
          withCredentials([usernamePassword(credentialsId: 'aws-s3', usernameVariable: 'AWS_ACCESS_KEY_ID', passwordVariable: 'AWS_SECRET_ACCESS_KEY')]) {
            dir(path_compress) {
              sh "aws s3 cp ${artifactName}-${version}.${extension} s3://${s3bucket}/${s3prefix}/"
            }
          }
        }
      }
    }
  }
  post {
    always {
      cleanWs()
    }
  }
}
